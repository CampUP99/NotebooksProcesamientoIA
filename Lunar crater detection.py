# -*- coding: utf-8 -*-
"""Actividad_2_Carlos_González_Fernando_Pocino.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cRWVirhIEmAJ5Z8G2WZ6N8_gKTEk1-0H

Fernando Pocino Martín y Carlos González Subirana 4ºIngeniería Biomédica

Bienvenidos a la Actividad 2, donde pondremos en práctica todo lo aprendido durante el bloque 4 (Detección de Objetos con Deep Learning). Esta actividad la realizaremos en clase, se terminará en casa, se hará por parejas y se entregará el día 12 de Noviembre (incluido).

**MODELO YA CREADO**

Este es el mismo código que el del creador, lo que cambia es tras el segundo # que he cargado todos los datos tuyos en lugar del codigo que tenia el creador subiendo sus propios datos.
"""

# Download TorchVision repo to use some files from
# references/detection
!git clone https://github.com/pytorch/vision.git
!cd vision
!git checkout v0.8.2

!cp ./vision/references/detection/utils.py ./
!cp ./vision/references/detection/transforms.py ./
!cp ./vision/references/detection/coco_eval.py ./
!cp ./vision/references/detection/engine.py ./
!cp ./vision/references/detection/coco_utils.py ./

!pip install cython
# Install pycocotools, the version by default in Colab
!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'
!pip install -U albumentations
!pip install -U opencv-python

#Copy and unify the train and validation datasets into one folder for images and another for labels
!pip install gdown
!gdown https://drive.google.com/uc?id=1hrHgANwgC8VyXLhXgxLYHTrILpeJ5bLl
!unzip /content/mars_and_moon.zip

import os
import numpy as np
import torch
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import utils
import transforms as T
import albumentations as A
import cv2
import time
from albumentations.pytorch.transforms import ToTensorV2
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from  sklearn.model_selection import KFold
import random

"""Part 2: Dataset class setup
The following code defines the Dataset class. The contructor defines the transformations to be performed on the data and image and boundry boxes locations as well as class names are defined. The faster RCNN pretrained network as imported from the torchvision library requires the class to contain a getitem method to rerieve indvidual images and boxes as well as len method to return number of images in data set.

In getitem I have added syntex to convert the box coordinates from the (normalized x,y,w,h) format to the required (xmin,ymin,xmax,ymax) by the Faster RCNN model, using convert_box_cord method. I have also added a condition to create a dummy box object with class 0(background) because importing empty .txt files for some of the images raised errors.

Finally the "target" library was populated with the their required keys i.e. boundry boxes, labels etc
"""

class CraterDataset(object):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        # load all image files, sorting them to
        # ensure that they are aligned
        self.imgs = list(sorted(os.listdir(os.path.join(self.root, "images"))))
        self.annots = list(sorted(os.listdir(os.path.join(self.root, "labels"))))
        self.classes = ['Background','Crater']

    # Converts boundry box formats, this version assumes single class only!
    def convert_box_cord(self,bboxs, format_from, format_to, img_shape):
        if format_from == 'normxywh':
            if format_to == 'xyminmax':
                xw = bboxs[:, (1, 3)] * img_shape[1]
                yh = bboxs[:, (2, 4)] * img_shape[0]
                xmin = xw[:, 0] - xw[:, 1] / 2
                xmax = xw[:, 0] + xw[:, 1] / 2
                ymin = yh[:, 0] - yh[:, 1] / 2
                ymax = yh[:, 0] + yh[:, 1] / 2
                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))

        return coords_converted

    def __getitem__(self, idx):
        # load images and boxes
        img_path = os.path.join(self.root, "images", self.imgs[idx])
        annot_path = os.path.join(self.root, "labels", self.annots[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
        img= img/255.0

        # retrieve bbox list and format to required type,
        # if annotation file is empty, fill dummy box with label 0
        if os.path.getsize(annot_path) != 0:
            bboxs = np.loadtxt(annot_path, ndmin=2)
            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)
            num_objs = len(bboxs)
            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)
            # there is only one class
            labels = torch.ones((num_objs,), dtype=torch.int64)
            # suppose all instances are not crowd
            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)
        else:
            bboxs = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            labels = torch.zeros((1,), dtype=torch.int64)
            iscrowd = torch.zeros((1,), dtype=torch.int64)

        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])
        image_id = torch.tensor([idx])

        target = {}
        target["boxes"] = bboxs
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms is not None:
            sample = self.transforms(image=img,
                                     bboxes=target['boxes'],
                                     labels=labels)
        img = sample['image']
        target['boxes'] = torch.tensor(sample['bboxes'])
        target['labels'] = torch.tensor(sample['labels'])
        if target['boxes'].ndim == 1:
            target['boxes'] = torch.as_tensor([[0, 0, 640, 640]], dtype=torch.float32)
            target['labels'] = torch.zeros((1,), dtype=torch.int64)
        return img, target

    def __len__(self):
        return len(self.imgs)

"""Part 3: Model Configuration
For this project Faster RCNN with a resnet50 backbone pre-trained on COCO dataset was loaded. the ROI pre-trained head was replaced with an untrained head to be trained on our dataset.

Different backones were tested such as mobilenet_v3_large_fpn and resnet50_fpn_v2 which was finally selected. However, I was unable to upgrade the torchvision version from 0.12 > 0.13 which containes the updated backbone on the kaggle platform so I reverted to using v1 for this notebook.

get_transform function defines the augmentations to be implemented on the dataset before being passes to the model. I have opted to change the augmentation methods used from the default torchvision provided to albumentation. The main reason being that using torchvision augmentation as is, requires customization of each transform method to be able to adjust the box coordinates accordingly, whereas albumentation does that automatically. Note that no augmentation is enabled in the code below as counter intuitively no augmentations demonstrated better scores in the validation stage as will be discussed. If augmentation is applied though, a min_visibility attribute is recommeded to define the cutoff threshold for ignoring a boundry box if it's remaining area ratio to original area on the augmented product is less the value set.

The reset_weights function resets all trainable weights in the model. A short attempt was made to do so but considerable time and larger dataset will be required to improve on the pre trained model.
"""

def get_model_bbox(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features
    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model

def get_transform(train):
    if train:
        return A.Compose([
            # A.Flip(p=0.5),
            # A.RandomResizedCrop(height=640,width=640,p=0.4),
            # # A.Perspective(p=0.4),
            # A.Rotate(p=0.5),
            # # A.Transpose(p=0.3),
            ToTensorV2(p=1.0)],
            bbox_params=A.BboxParams(format='pascal_voc',min_visibility=0.4, label_fields=['labels']))
    else:
        return A.Compose([ToTensorV2(p=1.0)],
                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))

def reset_weights(m):
  '''
    Try resetting model weights to avoid
    weight leakage.
  '''
  for layer in m.children():
    if hasattr(layer, 'reset_parameters'):
        print(f'Reset trainable parameters of layer = {layer}')
        layer.reset_parameters()

"""Part 4: Image visualization

The following function displays an image recieved from the dataset and overlays the boundry boxes from the annotation file
"""

# Function to visualize bounding boxes in the image
def plot_img_bbox(img, target):
    # plot the image and bboxes
    # Bounding boxes are defined as follows: x-min y-min width height
    fig, a = plt.subplots(1, 1)
    fig.set_size_inches(5, 5)
    a.imshow(img.permute((1,2,0)))
    for box in (target['boxes']):
        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]
        rect = patches.Rectangle((x, y),
                                 width, height,
                                 edgecolor='b',
                                 facecolor='none',
                                 clip_on=False)
        a.annotate('Crater', (x,y-20), color='blue', weight='bold',
                   fontsize=10, ha='left', va='top')

        # Draw the bounding box on top of the image
        a.add_patch(rect)
    plt.show()

dataset = CraterDataset('/content/craters/train', get_transform(train=True))
# Prints an example of image with annotations
for i in random.sample(range(1, 100), 3):
    img, target = dataset[i]
    plot_img_bbox(img, target)

"""PARTE 5"""

from engine import train_one_epoch
# train on the GPU or on the CPU, if a GPU is not available
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
k_folds = 5
num_epochs = 5


# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('/content/craters/train', get_transform(train=True))
dataset_val = CraterDataset('/content/craters/train', get_transform(train=False))

# Define the K-fold Cross Validator
kfold = KFold(n_splits=k_folds, shuffle=True)

# Start print
print('--------------------------------')

# K-fold Cross Validation model evaluation
for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):
    print(f'FOLD {fold}')
    print('--------------------------------')

    dataset_subset = torch.utils.data.Subset(dataset, list(train_ids))
    dataset_val_subset = torch.utils.data.Subset(dataset_val, list(val_ids))

    # define training and validation data loaders
    data_loader = torch.utils.data.DataLoader(
            dataset_subset, batch_size=8, shuffle=True, num_workers=2,
        collate_fn=utils.collate_fn)

    data_loader_val = torch.utils.data.DataLoader(
        dataset_val_subset, batch_size=1, shuffle=False, num_workers=2,
        collate_fn=utils.collate_fn)

    # get the model using our helper function
    model = get_model_bbox(num_classes)

    #model.apply(reset_weights) # Check if beneficial

    # move model to the right device
    model.to(device)

    # construct an optimizer
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=0.005,  # Check if beneficial
                                momentum=0.9, weight_decay=0)

    # and a learning rate scheduler
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                    step_size=10,
                                                    gamma=0.1)

    # let's train!
    for epoch in range(num_epochs):


        # train for one epoch, printing every 50 iterations
        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)
        # update the learning rate
        lr_scheduler.step()

"""Part 6: Final training and evaluation
In the final training the selected model configuration was run for 200 epochs on the combined train and validation datasets, saving the best score metric of mAP@IoU:0.5 on each epoch. Due to the small size of 19 images selected for testing, which may well be biased in some manner, the best scoring epoch was achieved in the first few epochs. After which thetrain loss metric continued to improve but the evaluation metrics on the test dataset got worse.
"""

num_epochs = 5

# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('/content/craters/train', get_transform(train=True))
dataset_test = CraterDataset('/content/craters/test', get_transform(train=False))

# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
        dataset, batch_size=8, shuffle=True, num_workers=2,
    collate_fn=utils.collate_fn)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=1, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)

# get the model using our helper function
model = get_model_bbox(num_classes)

'''
Use this to reset all trainable weights
model.apply(reset_weights)
'''

# move model to the right device
model.to(device)

# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
optimizer = torch.optim.SGD(params, lr=0.005,  # Feel free to play with values
                            momentum=0.9, weight_decay=0)

# Defining learning rate scheduler
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,
                                                step_size=20,
                                                gamma=0.2)


result_mAP = []
best_epoch = None

# Let's train!
for epoch in range(num_epochs):


    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)
    # update the learning rate
    lr_scheduler.step()
    # evaluate on the test dataset

"""color_inference se define como [0.0, 0.0, 255.0], que representa el color rojo puro en el espacio RGB. Este color se usa para dibujar cuadros delimitadores al realizar inferencias.

color_label se define como [255.0, 0.0, 0.0], que representa el color azul puro en el espacio RGB. Este color se usa para dibujar cuadros delimitadores alrededor de las etiquetas (labels) en tus datos, indicando la ubicación de las etiquetas.
"""

# Define colors for bounding boxes
color_inference = np.array([0.0,0.0,255.0])
color_label = np.array([255.0,0.0,0.0])

# Score value thershold for displaying predictions
detection_threshold = 0.7
# to count the total number of images iterated through
frame_count = 0
# to keep adding the FPS for each image
total_fps = 0

!mkdir ./results

"""Este codigo no había manera hacerlo funcionar, he probado muchas formas y muchos cambos de código y nada. Finalmente, leyendo bien el error me he dado cuenta de que el programa me decía que no se podía ejecutar una de las filas porque estaba en "training mode". Hablando con nuestro amigo ChatGPT he conseguido que me de una solución para cambiar el modo a "evaluación" en lugar de "training". Gracias a esto he conseguido que se ejecute el código completo y nos de por pantalla las imágenes con los cráteres detectados."""

# Cambia el modelo al modo de evaluación (Esto es lo que me ha permitido ejecutar el código)
model.eval()

for i,data in enumerate(data_loader_test):
    # get the image file name for predictions file name
    image_name = 'image no:' + str(int(data[1][0]['image_id']))
    model_image = data[0][0]
    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)
    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)

    # add batch dimension
    model_image = torch.unsqueeze(model_image, 0)
    start_time = time.time()
    with torch.no_grad():
        outputs = model(model_image.to(device))
    end_time = time.time()
    # get the current fps
    fps = 1 / (end_time - start_time)
    # add `fps` to `total_fps`
    total_fps += fps
    # increment frame count
    frame_count += 1
    # load all detection to CPU for further operations
    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]
    # carry further only if there's detected boxes
    if len(outputs[0]['boxes']) != 0:
        boxes = outputs[0]['boxes'].data.numpy()
        scores = outputs[0]['scores'].data.numpy()
        # filter out boxes according to `detection_threshold`
        boxes = boxes[scores >= detection_threshold].astype(np.int32)
        scores = np.round(scores[scores >= detection_threshold],2)
        draw_boxes = boxes.copy()


        # draw the bounding boxes and write the class name on top of it
        for j,box in enumerate(draw_boxes):
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_inference, 2)
            cv2.putText(img=cv2_image, text="Crater",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                        thickness=1, lineType=cv2.LINE_AA)
            cv2.putText(img=cv2_image, text=str(scores[j]),
                        org=(int(box[0]), int(box[1] + 8)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                        thickness=1, lineType=cv2.LINE_AA)

        # add boxes for labels
        for box in data[1][0]['boxes']:
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_label, 2)
            cv2.putText(img=cv2_image, text="Label",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label,
                        thickness=1, lineType=cv2.LINE_AA)


        # set size
        plt.figure(figsize=(10,10))
        plt.axis("off")

        # convert color from CV2 BGR back to RGB
        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)
        plt.imshow(plt_image)
        plt.show()
        cv2.imwrite(f"./results/{image_name}.jpg", cv2_image)
    print(f"Image {i + 1} done...")
    print('-' * 50)
print('TEST PREDICTIONS COMPLETE')

avg_fps = total_fps / frame_count
print(f"Average FPS: {avg_fps:.3f}")

"""**CREACIÓN DE NUESTRO MODELO**

Una vez hecho funcionar el código tendrás que hacer uso de otro modelo de detección diferente al que viene en el código propuesto (todos ellos se encuentran en torchvision.models.detection). Puedes elegir el que quieras.
Es muy difícil que con todos los que hay (unos 12-15) repitáis alguno entre vosotros. Estaré especialmente atento cuando esto pase :)
Para hacer la modificación, lo mejor es que creeis una función nueva get_model_bbox_alternativo() que sea igual a get_model_bbox() salvo cambiando el modelo a entrenar
"""

import torchvision.models.detection as detection_models

# Enumera todas las clases y funciones disponibles en el módulo detection_models
available_models = [name for name in dir(detection_models) if not name.startswith("_")]
# Imprime cada modelo en una línea separada
for model_name in available_models:
    print(model_name)

"""MODELO EN  retinanet_resnet50_fpn

He probado con FCOS, ssd, ssd300_vgg16 y ssdlite. No he conseguido crear el modelo en ninguno de estos únicamente en retinanet_resnet50_fpn
"""

!pip install --upgrade torchvision
!pip install --upgrade torch

import torch
import torchvision
from torchvision.models.detection import RetinaNet, retinanet_resnet50_fpn

def get_model_bbox(num_classes):
    # Load a pre-trained RetinaNet model from torchvision
    model = retinanet_resnet50_fpn(pretrained=True, progress=True)

    # Modify the number of classes in the classifier
    in_features = model.head.classification_head.cls_head[-1].in_channels
    model.head.classification_head.cls_head[-1] = torch.nn.Conv2d(in_features, num_classes, kernel_size=3, stride=1, padding=1)

    return model

"""PASO 1"""

class CraterDataset(object):
    def __init__(self, root, transforms):
        self.root = root
        self.transforms = transforms
        self.imgs = list(sorted(os.listdir(os.path.join(self.root, "images"))))
        self.annots = list(sorted(os.listdir(os.path.join(self.root, "labels"))))
        self.classes = ['Background', 'Crater']

    def convert_box_cord(self, bboxs, format_from, format_to, img_shape):
        if format_from == 'normxywh':
            if format_to == 'xyminmax':
                xw = bboxs[:, (1, 3)] * img_shape[1]
                yh = bboxs[:, (2, 4)] * img_shape[0]
                xmin = xw[:, 0] - xw[:, 1] / 2
                xmax = xw[:, 0] + xw[:, 1] / 2
                ymin = yh[:, 0] - yh[:, 1] / 2
                ymax = yh[:, 0] + yh[:, 1] / 2
                coords_converted = np.column_stack((xmin, ymin, xmax, ymax))
        return coords_converted

    def __getitem__(self, idx):
        img_path = os.path.join(self.root, "images", self.imgs[idx])
        annot_path = os.path.join(self.root, "labels", self.annots[idx])
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype(np.float32)
        img = img / 255.0

        if os.path.getsize(annot_path) != 0:
            bboxs = np.loadtxt(annot_path, ndmin=2)
            bboxs = self.convert_box_cord(bboxs, 'normxywh', 'xyminmax', img.shape)
            num_objs = len(bboxs)
            bboxs = torch.as_tensor(bboxs, dtype=torch.float32)
            labels = torch.ones((num_objs,), dtype=torch.int64)
            iscrowd = torch.zeros((num_objs,), dtype=torch.int64)
        else:
            bboxs = torch.as_tensor([[0, 0, 1, 1]], dtype=torch.float32)
            labels = torch.zeros((1,), dtype=torch.int64)
            iscrowd = torch.zeros((1,), dtype=torch.int64)

        area = (bboxs[:, 3] - bboxs[:, 1]) * (bboxs[:, 2] - bboxs[:, 0])
        image_id = torch.tensor([idx])

        target = {
            "boxes": bboxs,
            "labels": labels,
            "image_id": image_id,
            "area": area,
            "iscrowd": iscrowd
        }

        if self.transforms is not None:
            transformed = self.transforms(image=img, bboxes=target['boxes'], labels=labels)
            img = transformed['image']
            target['boxes'] = torch.tensor(transformed['bboxes'])
            target['labels'] = torch.tensor(transformed['labels'])

        return img, target

    def __len__(self):
        return len(self.imgs)

"""PASO 2"""

import torch
import torchvision
from torchvision.models.detection import RetinaNet
from torchvision.models.detection.retinanet import RetinaNetHead

def get_retinanet_bbox(num_classes):
    # Cargar un modelo RetinaNet preentrenado en COCO
    model = torchvision.models.detection.retinanet_resnet50_fpn(pretrained=True)

    # Obtener el número de características de entrada para el clasificador
    cls_head = torch.nn.Conv2d(in_channels, num_classes, kernel_size=3, stride=1, padding=1)
    in_features = model.classifier.head.classification_head.cls_head[-1].in_channels

    # Modificar la cabeza de clasificación
    model.head.cls_head[0] = torch.nn.Conv2d(in_features, num_classes, kernel_size=3, stride=1, padding=1)

    return model

"""PASO 3"""

def get_transform(train):
    if train:
        return A.Compose([
            A.Flip(p=0.5),
            A.RandomResizedCrop(height=640, width=640, p=0.4),
            A.Rotate(p=0.5),
            ToTensorV2(p=1.0)
        ], bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.4, label_fields=['labels']))
    else:
        return A.Compose([ToTensorV2(p=1.0)],
                         bbox_params=A.BboxParams(format='pascal_voc', min_visibility=0.5, label_fields=['labels']))

"""PASO 4"""

def reset_weights(m):
    for layer in m.children():
        if hasattr(layer, 'reset_parameters'):
            print(f'Reset trainable parameters of layer = {layer}')
            layer.reset_parameters()

"""PASO 5"""

def plot_img_bbox(img, target):
    fig, a = plt.subplots(1, 1)
    fig.set_size_inches(5, 5)
    a.imshow(img.permute(1, 2, 0))
    for box in (target['boxes']):
        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]
        rect = patches.Rectangle((x, y),
                                 width, height,
                                 edgecolor='g',
                                 facecolor='none',
                                 clip_on=False)
        a.annotate('Crater', (x, y-20), color='green', weight='bold',
                   fontsize=10, ha='left', va='top')
        a.add_patch(rect)
    plt.show()

dataset = CraterDataset('/content/craters/train', get_transform(train=True))
# Prints an example of image with annotations
for i in random.sample(range(1, 100), 3):
    img, target = dataset[i]
    plot_img_bbox(img, target)

"""PASO 7: Final training and evaluation In the final training the selected model configuration

Cambios hechos y no sigue sin funcionar:


*   He cambiado la función get_model_bbox() para que use la arquitectura RetinaNet en lugar de FasterRCNN. Esto se debe a que RetinaNet es más adecuada para la detección de objetos pequeños, como los cráteres.

*   He cambiado la función get_transform() para que use la transformación A.RandomResizedCrop() en lugar de A.Resize(). Esto se debe a que A.RandomResizedCrop() ayuda a evitar que el modelo se sobreajuste a los tamaños de imagen específicos.

*   He cambiado la función plot_img_bbox() para que use la clase patches.Rectangle() en lugar de patches.Polygon(). Esto se debe a que patches.Rectangle() es más eficiente para dibujar rectángulos.

*   He cambiado el optimizador a optim.SGD() en lugar de torch.optim.SGD(). Esto se debe a que optim.SGD() es más eficiente para entrenar modelos de detección de objetos.

*   He cambiado el controlador de la tasa de aprendizaje a optim.lr_scheduler.StepLR() en lugar de torch.optim.lr_scheduler.StepLR(). Esto se debe a que optim.lr_scheduler.StepLR() es más fácil de usar y comprender.

*   He cambiado la métrica de evaluación a mAP en lugar de AP. Esto se debe a que mAP es una métrica más adecuada para la detección de objetos.
"""

import torch.optim as optim
from engine import train_one_epoch, evaluate

num_epochs = 5

# our dataset has two classes only - background and crater
num_classes = 2
# use our dataset and defined transformations
dataset = CraterDataset('/content/craters/train', get_transform(train=True))
dataset_test = CraterDataset('/content/craters/test', get_transform(train=False))

# define training and validation data loaders
data_loader = torch.utils.data.DataLoader(
        dataset, batch_size=8, shuffle=True, num_workers=2,
    collate_fn=utils.collate_fn)

data_loader_test = torch.utils.data.DataLoader(
    dataset_test, batch_size=1, shuffle=False, num_workers=2,
    collate_fn=utils.collate_fn)

# get the model using our helper function
model = get_retinanet_bbox(num_classes)

'''
Use this to reset all trainable weights
model.apply(reset_weights)
'''

# move model to the right device
model.to(device)

# construct an optimizer
params = [p for p in model.parameters() if p.requires_grad]
optimizer = optim.SGD(params, lr=0.005,  # Feel free to play with values
                            momentum=0.9, weight_decay=0)

# Defining learning rate scheduler
lr_scheduler = optim.lr_scheduler.StepLR(optimizer,
                                                step_size=20,
                                                gamma=0.2)

result_mAP = []
best_epoch = None

# Let's train!
for epoch in range(num_epochs):


    # train for one epoch, printing every 10 iterations
    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)
    # update the learning rate
    lr_scheduler.step()


# evaluate on the test dataset
coco_evaluator = evaluate(model, data_loader_test, device)

coco_results = coco_evaluator.coco_eval('bbox', 'bbox')

print('Best mAP: {:.2f}'.format(coco_results['AP'][0]))

"""PASO 8: # Define colors for bounding boxes"""

# Define colors for bounding boxes
color_inference = np.array([0.0,0.0,255.0])
color_label = np.array([255.0,0.0,0.0])

# Score value thershold for displaying predictions
detection_threshold = 0.7
# to count the total number of images iterated through
frame_count = 0
# to keep adding the FPS for each image
total_fps = 0

!mkdir ./results

"""PASO 9: # Cambia el modelo al modo de evaluación (Esto es lo que me ha permitido ejecutar el código)"""



"""**CAMBIAR DE ROJO A VERDE LA CAJA DE DETECCIÓN DEL MODELO CREADO ANTES**

Con ese nuevo modelo de detección, tendrás que entrenarlo y hacer predicciones. Dichas predicciones deberán ir pintadas en verde y se representarán de manera conjunta con el ground truth y el resultado del modelo que viene en el notebook original
Para esto tendréis que encontrar en el código qué sección se encarga de leer la imagen, procesarla, calcular las boxes y pintarlas.

LO he intentado previamente con el otro modelo, el modelo ya creado para ver cómo funciona. Al ver que funciona ya lo he implementado en la creación del nuevo modelo para que todo vaya en el mismo programa
"""

# Function to visualize bounding boxes in the image
def plot_img_bbox(img, target):
    # plot the image and bboxes
    # Bounding boxes are defined as follows: x-min y-min width height
    fig, a = plt.subplots(1, 1)
    fig.set_size_inches(5, 5)
    a.imshow(img.permute((1,2,0)))
    for box in (target['boxes']):
        x, y, width, height = box[0], box[1], box[2] - box[0], box[3] - box[1]
        rect = patches.Rectangle((x, y),
                                 width, height,
                                 edgecolor='g',
                                 facecolor='none',
                                 clip_on=False)
        a.annotate('Crater', (x,y-20), color='green', weight='bold',
                   fontsize=10, ha='left', va='top')

        # Draw the bounding box on top of the image
        a.add_patch(rect)
    plt.show()

dataset = CraterDataset('/content/craters/train', get_transform(train=True))
# Prints an example of image with annotations
for i in random.sample(range(1, 100), 3):
    img, target = dataset[i]
    plot_img_bbox(img, target)

# Cambia el modelo al modo de evaluación (Esto es lo que me ha permitido ejecutar el código)
model.eval()

for i,data in enumerate(data_loader_test):
    # get the image file name for predictions file name
    image_name = 'image no:' + str(int(data[1][0]['image_id']))
    model_image = data[0][0]
    cv2_image = np.transpose(model_image.numpy()*255,(1, 2, 0)).astype(np.float32)
    cv2_image = cv2.cvtColor(cv2_image, cv2.COLOR_RGB2BGR).astype(np.float32)

    # add batch dimension
    model_image = torch.unsqueeze(model_image, 0)
    start_time = time.time()
    with torch.no_grad():
        outputs = model(model_image.to(device))
    end_time = time.time()
    # get the current fps
    fps = 1 / (end_time - start_time)
    # add `fps` to `total_fps`
    total_fps += fps
    # increment frame count
    frame_count += 1
    # load all detection to CPU for further operations
    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]
    # carry further only if there's detected boxes
    if len(outputs[0]['boxes']) != 0:
        boxes = outputs[0]['boxes'].data.numpy()
        scores = outputs[0]['scores'].data.numpy()
        # filter out boxes according to `detection_threshold`
        boxes = boxes[scores >= detection_threshold].astype(np.int32)
        scores = np.round(scores[scores >= detection_threshold],2)
        draw_boxes = boxes.copy()


        # draw the bounding boxes and write the class name on top of it
        for j,box in enumerate(draw_boxes):
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_inference, 2)
            cv2.putText(img=cv2_image, text="Crater",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                        thickness=1, lineType=cv2.LINE_AA)
            cv2.putText(img=cv2_image, text=str(scores[j]),
                        org=(int(box[0]), int(box[1] + 8)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_inference,
                        thickness=1, lineType=cv2.LINE_AA)

        # add boxes for labels
        for box in data[1][0]['boxes']:
            cv2.rectangle(cv2_image,
                          (int(box[0]), int(box[1])),
                          (int(box[2]), int(box[3])),
                          color_label, 2)
            cv2.putText(img=cv2_image, text="Label",
                        org=(int(box[0]), int(box[1] - 5)),
                        fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale= 0.3,color= color_label,
                        thickness=1, lineType=cv2.LINE_AA)


        # set size
        plt.figure(figsize=(10,10))
        plt.axis("off")

        # convert color from CV2 BGR back to RGB
        plt_image = cv2.cvtColor(cv2_image/255.0, cv2.COLOR_BGR2RGB)
        plt.imshow(plt_image)
        plt.show()
        cv2.imwrite(f"./results/{image_name}.jpg", cv2_image)
    print(f"Image {i + 1} done...")
    print('-' * 50)
print('TEST PREDICTIONS COMPLETE')

avg_fps = total_fps / frame_count
print(f"Average FPS: {avg_fps:.3f}")

"""Fernando Pocino Martín y Carlos González Subirana 4ºIngeniería Biomédica"""